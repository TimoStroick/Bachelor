\newpage
\section{Extraktion}


Der vorliegende Abschnitt bezieht sich auf das Extrahieren von Daten. Dieser Ablauf unterteilt sich in Parsen, um die Daten zu Beginn von der DBLP zu extrahieren. Dabei werden diese in passende Entitäten umgewandelt, um sie für die Datenbank nutzen zu können. Um diese danach zu speichern, wird die Datenbank mit diesen Entitäten entworfen. 

Im ersten Teil werden die Daten von der \textit{DBLP} extrahiert. Dafür werden zunächst die Daten, die zu extrahieren sind, analysiert. Die \textit{DBLP} bietet dafür zwei Möglichkeiten an: Die erste ist eine Request API bzw. Schnittstelle. 

Durch diese API können Anfragen an den Server gesendet werden. Dieser reagiert auf die Anfrage und schickt die Informationen zurück, die durch die Anfrage verlangt wurden. Damit ist es möglich, Daten von der DBLP zu erhalten. Nun kann die Anfrage in dieser API nur bestimmte Informationen abfragen, wie beispielsweise bestimmte Artikel, Autoren und Arbeiten von Autoren. Damit sind nur reduzierte Datenausgaben möglich.





Für diesen Fall ist die zweite Möglichkeit besser geeignet, auf Grund der Tatsache, dass ein Dump File (dblp.xml) genutzt wird. Ein Dump File ist ein Ausdruck, von dem gesamten Inhalt eines Speichers – hier von einer Datenbank. Das File enthält Daten, die die Datenbank zudem Zeitpunkt besitzt. Da sich die Datenbank regelmäßig ändert oder erweitert, erstellt die DBLP regelmäßig neue Dump Files. Das File was benutzt wird, ist vom 13. Mai 2020. Es kann viele Dateiformate besitzen, wobei es sich hier um ein XML-Format handelt. 
Hierzu gibt es eine Dokumentation, die die Formatierung des XML-Formates beschreibt. Da diese schon \citetitle{DBLP:journals/pvldb/Ley09}(\cite{DBLP:journals/pvldb/Ley09}) heißt, lässt es darauf schließen, dass einige Fehler bei dem ursprünglichen Design aufgetreten sind und einige Änderungen gemacht wurden. Die erste Dokumentation der DBLP ist vom 18. Juni 2009. Zu diesem Zeitpunkt waren nur 532MP in der XML-Datei. Im Gegensatz dazu sind nun 2.806MB in der Datei enthalten, demnach das Fünffache der ursprünglichen Datei. Durch das Wachstum wurden einige Anpassungen getätigt, die von der Dokumentation abweichen. Somit muss die Formatierung der Datenstrukturen an Hand der Datei selbst herausgefunden werden, da nicht genau festgelegt war, was eine Publikation als Daten enthalten kann. Dies führt zu vielen optionalen Eigenschaften, da nicht alle Daten vom selbem Type die selben Eigenschaften besitzen. 

Für das Extrahieren der Daten wird daraufhin ein Parser benötigt, der die XML-Datei ausliest. Die meisten Parser lesen die Datei komplett ein und geben die XML-Daten dann heraus. Da die Datei nun 2,8 GB groß ist, laufen die meisten Parser an ihre Grenzen. Dazu kommt, dass das Dump File in den nächsten Jahren beliebig groß werden kann. Deshalb sind Stream basierte Parser die einzige Lösung. Es gibt damit zwar Parser, mit denen es funktionieren würde, aber in diesem Fall wurde sich bewusst dagegen entschieden. 

Daher wird ein eigener Parser geschrieben, um dieses Problem zu umgehen. Durch diesen regulären Ausdruck aus der Dokumentation wird gezeigt, dass es nur eine bestimmte Anzahl an Elementen gibt, die verwendet werden. 

\begin{center}
	<!ELEMENT dblp (article|inproceedings| proceedings|book|incollection|phdthesis|mastersthesis|www)*>
\end{center}



Das ist ein Ausschnitt aus der Dokumentation(vgl. \cite{DBLP:journals/pvldb/Ley09}, S. 2) und beschreibt das Format der XML. Dieser zeigt alle Elemente (Tags), die in der Datei vorhanden sind. Nach dem testen traten keine anderen Elemente auf. So werden die benötigten Fähigkeiten, die der Parser haben muss, genau festgelegt. 



\subsection{Parser}

Dieses Programm wurde in Python geschrieben. Python ist eine interpretierte, interaktive und objektorientierte Programmsprache.(vgl. \cite{Pythonwasistdas}) Diese Sprache basiert nicht wie üblich auf Semikola als Trennung, sondern basiert auf Spalten. Damit hat jedes Einrücken eine Bedeutung. 

Alle Klassen folgen dem Single Responsibility Prinzip, welches besagt, dass jede Klasse nur eine Aufgabe verfolgt. So werden auch alle Klassen angelegt und geschrieben. Demnach extrahiert die Parser-Klasse die Daten, die DBParser-Klasse teilt die Daten in Entitäten auf und die Insert-Klasse fügt die Daten in die Datenbank ein. So ist das Programm besser wartbar und gibt ein sauberes Programmbild ab. 

Bevor der Parser erklärt wird, muss zuerst das Dateiformat XML weiter ausgeführt werden. XML bedeutet Extensible Markup Language und ist eine erweiterbare Auszeichnungssprache, mit deren Hilfe Daten im Textformat gespeichert werden können. 


Dies funktioniert, indem mit einem Tag einem bestimmtem Element einen Namen oder eine Kategorie zugeteilt wird, wie bspw. ‚<author> Timo Stroick </author>’. Hierbei bildet alles zusammen ein Element, und das eingeklammerte ‚author‘ ein Tag. Dabei ist darauf zu achten, dass Elemente immer mit dem dazugehörigen Endtag schließen muss, wie anhand des Beispiels gezeigt. 
Eine weitere Möglichkeit ist es, Elemente in Elemente zu schreiben. Hier gibt es eine besondere Regel, die XML ausmacht. Elemente, die in andere Elemente geschrieben sind, müssen erst geschlossen werden, bevor die äußeren geschlossen werden dürfen. Durch diese Regeln entsteht die typische hierarchische Struktur des XML-Dateiformats. In der dblp.xml sind alle Daten in ein Tag eingeschlossen, welches dblp heißt. So fängt die Datei bei <dblp> an und enden bei </dblp>. Somit sind alle wichtigen Daten zwischen den beiden Tags. 

 
\begin{figure}[!htb]
	\centering
	\includegraphics[width=14cm,keepaspectratio]{bilder/xmlAusschnitt}
	\caption{XML-Ausschnitt}
	\label{fig:xmlausschnitt}
	Ein Beispiel Ausschnitt wie ein typischer Eintrag in der dblp.xml aussieht. Dieser wurde verkürzt und angepasst.
\end{figure}

Der Parser ist in einer eigenen Klasse. Um mit dem Parser zu beginnen, werden die Daten zeilenweise in der Klasse eingelesen, um das Problem mit der Dateigröße zu umgehen. Denn würde alles auf einmal eingelesen werden, würde der Speicher des Programms schnell voll werden und damit den Computer überlasten. Nun wird in den Zeilen nach den Tags gesucht. Ist ein Tag gefunden worden, wird dessen Name überprüft und reagiert mit einem bestimmten Fall. Durch das zeilenweise Einlesen ist dies allerdings kein richtiger Streamparser, denn dieser würde Zeichen für Zeichen einlesen, um die Datei zu parsen. 

Da durch die Dokumentation alle Tags bekannt sind, die in der Datei auftreten können, müssen nur acht Fälle behandelt werden. Somit haben wir nur acht Methoden, mit denen wir jeden Fall abdecken. Alle von denen arbeiten genau ein Element ab. Jede Methode liest weitere Zeilen und Daten ein, bis das Endtag des gewissen Elements, die Methode beendet. Das zählt natürlich auch für das dblp-Endtag, welchen den Parser beendet. Die in der Zeit eingelesenen Tags und Werte sind die Attribute des entsprechenden Elements, wie bspw. bei ‚article‘, bei dem es der Autor sein kann, der eingelesen wird. 









Die Daten werden nun in eine weitere Klasse übergeben. Diese Klasse ist der DBParser, in diesem die Daten nochmals angepasst werden. Da die erste Klasse die Daten aus der XML-Datei in Daten in das Programm wandelt, wird diese Klasse nun die Daten so anpassen, sodass sie für die Datenbank nutzbar werden. Um nachher die Datenbank ordentlich verwenden zu können, werden die acht Elemente in Entitäten gewandelt, die vorteilhafter für das Datenbankdesign sind. Entitäten sind Objekte oder Wesen, die in der Datenbank mit Attributen und Werten befüllt werden können. Diese Einteilung wird im weiteren Verlauf noch gezeigt und erläutert. 

Zum Schluss werden in der Insert Klasse die Daten nur noch in die Datenbank eingefügt. Somit sind alle Daten von einem Element so gespeichert, dass der Arbeitsspeicher erneut benutzt werden kann. Damit wird dafür gesorgt, dass nur ein Element gleichzeitig bearbeitet wird und kein Speicherüberlauf auftritt. 

 
\begin{figure}[!htb]
	\centering
	\includegraphics[width=15cm,keepaspectratio]{bilder/SequenzDiagramParser}
	\caption{Sequenzdiagramm-Auschnitt}
	\label{fig:sequenzdiagramm}
\end{figure}



Abbildung \ref{fig:sequenzdiagramm} zeigt einen typischen Programmverlauf eines Parsers. In diesem Beispielablauf wird ein ‚article‘ Element eingelesen. Zunächst wird die Parser-Klasse mit der startParser-Funktion in der Main-Methode aufgerufen und liest einen Tag nach dem anderen, bis er auf Treffer stößt. In diesem Fall ist das Tag ‚article‘ ein Treffer. Nun wird die parseArticle-Methode in der selben Klasse aufgerufen. In der Methode werden die darauffolgenden Elemente ausgelesen und deren Daten zwischengespeichert. Dies geschieht solange, bis das Endtag von ‚article‘ wieder eingelesen wird. Jetzt werden alle Daten an die saveArticle-Methode der DBParser-Klasse übergeben. In dieser Klasse werden nun die Daten an die Datenbank angepasst. Denn in dieser Datenbank gibt es keine Entität, die Artikel heißt. Das wird erst im nächsten Schritt erklärt. Im Allgemeinen wird der Artikel sowohl in Publikation, Autoren, elektronische Version und Fachzeitschrift als auch in die Beziehungen zwischen diesen Entitäten zerteilt. Da die Daten richtig zerlegt sind, werden sie an die Insert-Klasse einzeln übergeben. Das geschieht über die insertPublikation-Methode.


Jetzt sind alle Daten vorhanden und richtig zugewiesen. Daraufhin werden sie mit SQL in eine Postgres Datenbank gespeichert. Postgres ist ein freies objektrelationales Datenbankmanagementsystem (ORDBMS) und und basiert auf SQL.
SQL bedeutet Structured Query Language, also strukturierte Abfragesprache, und stellt einen Standard für relationale Datenbanken dar. Es gibt viele SQL-Datenbanken, wie zum Beispiel Amazon DynamoDB, IBM DB2, MongoDB, PostgresSQL u.Ä. 

PostgresSQL wird verwendet, da es sich zum einen um eine freie Oben-Source Datenbank handelt und zum anderen keine Lizenzen benötigt. Dazu hält sie weites gehend die SQL-Standards und Normen ein. Diese werden benötigt, um die Aufgabe ordentlich zu erfüllen. In den Methoden ist nämlich darauf zu achten, dass nur triviale SQL Befehle benutzt werden und keine speziellen eines bestimmten Datenbanktyps. Deshalb wird sich innerhalb dieser Arbeit auf Insert-Befehle und Select-Abfragen fokussiert, um auf keinen bestimmten Datenbanktypen angewiesen zu sein. 

Schlussendlich wird der Primärschlüssel der Publikation zurückgegeben, um die Beziehungen zwischen den Entitäten und der Datenbank einzutragen. Wenn nun die saveArticle-Methode zu Ende läuft und sich erfolgreich beendet, sucht die startparser-Methode nach dem nächsten Tag. So wird jedes Element nacheinander in die Datenbank eingespeichert, bis das Endtag von dblp eingelesen wird. 


\subsection{Datenbank}

Zur Erstellung der Datenbank werden zunächst die Daten analysiert. Die \textit{DBLP} gibt schon mit acht Elementen (article, inproceedings, proceedings, book, incollection, phdthesis, mastersthesis, www) einen groben Überblick. Diese Elemente sind allerdings noch nicht in dem relationalen Datenbankformat. Deshalb werden diese, wie vorhin erläutert, nochmal in kleinere Entitäten zerteilt. 
Für die Einteilung wird der \textit{Microsoft Graph} aus Abbildung \ref{fig:magschema} als Inspiration genommen, da bei dieser Zerkleinerung die Entitäten in ihrem fachlichen Kontext belassen werden müssen:

‚article‘ - Es setzt sich aus einer Fachzeitschrift, einer Publikation und mehreren elektronischen Versionen zusammen.

‚proceedings’ - Dies ist in diesem Zusammenhang eine Konferenz, die gehalten wurde
und besitzt Autoren, die diese Sammlung von Arbeiten angepasst haben. Diese Konferenz
kann auch ISBN’s enthalten.

‚inproceedings’ - Passend zu Konferenzen und ist eine Publikation, die in einer Konferenz veröffentlicht wurde.

‚book’ - Book ist ein Buch, mit ein oder mehreren Autoren. Hierzu gehören auch mehrere ISBN’s.

‚incollection’ - Dies sind Publikationen, die in einem Buch veröffentlicht wurden.

‚masterthesis’ - Die Masterthesis ist eine Publikation, die an einer Universität geschrieben wurde.

‚phdthesis’ - Im Gegensatz zur Masterthesis sind die phdthesis immer als Buch veröffentlicht.

‚www’ - Dies sind Homepages von Autoren, die abgegeben werden können.

Jede Publikation in den Erklärungen besitzt zusätzlich Autoren, die hier nicht aufgelistet sind. Dazu kommt noch, dass jeder Punkt elektronische Versionen enthält. Da in der \textit{DBLP} Bücher sind, die mehrere ISBN’s haben, muss eine extra Entität für die ISBN erstellt werden. Das Gleiche zählt für die elektronische Version, denn diese existiert für die meisten Entitäten mehrfach. Daraus ergeben sich dann diese Entitäten Autor, Publikation, Homepage, Konferenz, Fachzeitschrift, Buch, elektronische Version, ISBN und deren Beziehungen. Zu diesen Entitäten gehören ebenso die Attribute, die schon mit dem Parser eingelesen werden. Da nicht für jede Entität immer die gleiche Anzahl an Eigenschaften übergeben wird, werden die meisten Attribute optional. Mit den Entitäten und deren Attribute wird dann ein ER-Modell modelliert und an die entsprechenden Beziehungen angepasst.



\subsubsection{ER-Modell}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=15cm,keepaspectratio]{bilder/ER-Modell}
	\caption{ER-Modell}
	\label{fig:er-modell}
\end{figure}


Das ER-Modell veranschaulicht die Beziehungen zwischen den Entitäten. Die Entitäten
sind hier die Rechtecke. Die Attribute sind Kreise und direkt mit den Entitäten verbunden, zu denen sie gehören. Beziehungen werden durch Rauten dargestellt und verbinden immer zwei Entitäten oder nur eine Entität mit sich selbst (siehe Beziehung hat-Zitat).
Ein Zitat verbindet eine Publikation, mit der Publikation, die zitiert wurde. Es wird deutlich, dass Beziehungen auch Attribute haben können, denn es gibt Fälle, wo dieses Attribut zu keinem von beiden passt. Das Attribut passt nur, wenn beide in Beziehung zueinander stehen. 
Die Relation zwischen Publikation und Buch besitzt sie das Attribut Seiten. Dies macht nur im einzelnen Zusammenhang Sinn, da Seiten allein in einer Publikation ohne Buch keinen Anhaltspunkt besitzen. Andersherum kann ein Buch mehrere Publikationen beinhalten und deshalb nicht verschiedene Seitenangaben für verschiedene Publikationen speichern. Somit macht die Angabe von Seiten nur in der Beziehung der beiden Entitäten Sinn. Mit Attributen werden Entitäten beschrieben und es gibt Attribute, die für eine eindeutige Identifikation der Entität sorgen. Diese Attribute
werden unterstrichen und nachher zu Schlüsselkandidaten. Die eindeutige Eigenschaft
bei Autor ist der Key. Dieser Key sieht wie in Abbildung \ref{fig:xmlausschnitt}. aus und ist der komplette Name eines Autors. Falls dieser Name doppelt vorkommt, haben die beiden Autoren eine zusätzliche Nummer hinter ihrem Namen. Dadurch wir dieser wieder ein Unikat.

Dieses Modell muss nun umgewandelt werden. Da in der Datenbank nur Tabellen angelegt werden können, muss dieses ER Modell in die Form von Tabellen gebracht werden. Diese Form ist dann das relationale Schema. Dieses Schema beschreibt genau das Gleiche, wie das ER-Modell, nur hier werden Fremdschlüssel und damit Beziehungen angegeben.


\subsubsection{Relationales Schema}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=12cm,keepaspectratio]{bilder/relationalesSchema}
	\caption{Relationales Schema}
	\label{fig:relationalesSchema}
\end{figure}


Abbildung \ref{fig:relationalesSchema} zeigt das ER-Modell, welches genau in das relationale Schema übertragen wurde. Der erste Name ist der Name der Tabelle und alle Namen in der Klammer sind deren Attribute. Es wird deutlich, dass auch Beziehungen zwischen den Entitäten eine eigene Tabelle bekommen. Diese sorgen dafür, dass Entitäten mit einander verbunden werden. Die unterstrichenen Attribute stellen die Primärschlüssel da. Primärschlüssel sind Attribute, womit die Daten dieser Entität genau unterscheiden werden können, wie bspw. die Studierendennummer, die sich bei jedem* Studenten* unterscheidet. Gibt es keine eindeutige Eigenschaft, so wird ein künstlicher Schlüssel erstellt. In diesem Fall heißen alle künstlichen
Schlüssel Id. Diese Id wird mit jeder Erstellung eines neuen Eintrags erhöht, damit
diese eindeutig bleibt. Die Attribute die mit Punkten unterstrichen wurden, sind Fremdschlüssel. Diese Schlüssel sind Primärschlüssel aus anderen Tabellen.
Um sich diese besser vorstellen zu können, wird ein Beispiel angeführt:

Es gibt 2 Entitäten, Schüler und Klasse. Der Primärschlüssel für die Klasse wäre die Klassen Bezeichnung, wie 2a oder 1b. Der Schüler hat eine Id als Primärschlüssel, da der Name nicht ganz eindeutig ist. Um nun die Relation so darzustellen, dass der Schüler in eine Klasse geht, wird den Klassen Schlüssel an die Schüler als Fremdschlüssel angefügt. Nun steht in der Schülertabelle bspw.:

Name: Max Mustermann und Klasse: 3a. 

Somit dient der Fremdschlüssel dafür dass Entitäten verbunden werden können.
Für das erste wird in dem Schema jede Beziehung als Tabelle mit zwei Fremdschlüsseln
erstellt. Die Schlüssel kommen aus den Entitäten, die sie verbinden. Damit stellen sie eine eindeutige Verbindung zwischen Entitäten her.

Da hier noch überflüssige Tabellen enthalten sind, werden die Tabellen verschmolzen.



\subsubsection{Verschmolzenes relationale Schema}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=13cm,keepaspectratio]{bilder/verschmolzenesRelationaleSchema}
	\caption{Verschmolzenes Relationale Schema}
	\label{fig:verschmolzenesrelationalesSchema}
\end{figure}






Abbildung \ref{fig:verschmolzenesrelationalesSchema} zeigt nun das verschmolzene relationale Schema. Einige Tabellen sehen jetzt anders aus. Zum Beispiel verschwindet die Tabelle Buch-hat-ISBN ganz, denn der Primärschlüssel vom Buch steht nun direkt in der ISBN, da es nur ein Buch für eine ISBN geben kann. Dasselbe passiert mit Konferenz und ISBN. Dies ist eine N zu 1 Beziehung gewesen, da Fälle auftreten, wo Bücher mehrere ISBN’s haben. Die meisten anderen Beziehungen sind N zu M Relationen, wie bspw. Autor und Publikation. Jeder Autor kann mehrere Publikationen schreiben und andersherum, denn Publikationen können von mehreren Autoren geschrieben werden. Diese Relationen können nicht verschmolzen werden. Der letzte Fall wäre eine 1 zu 1 Relation, aber die kommt in diesem Modell nicht vor. Ein typisches Beispiel ist eine Person und ihr Ausweis, denn eine Person hat nur einen Ausweis und ein Ausweis darf nur einer Person gehören.


\subsubsection{Normalisierung}

Normalisierungen sind dafür da, Fehler und Redundanzen zu verhindern. Redundanzen
sind doppelt oder mehrfach vorkommende Informationen. Diese können bei großen
Datenbanken für viel unnötigen Platzverbrauch sorgen. Fehler führen dazu, dass die Datenbank abstürzt oder Befehle verweigert werden. Die Normalisierung werden in Stufen von Normalformen getätigt.

Je höher die Stufe der Normalform, desto weniger Anomalien können auftreten. Anomalien sind die Ergebnisse von genau diesen Redundanzen, die denn Umgang mit der Datenbank erschweren oder sogar ganz stoppen. Es können Daten gelöscht werden, die noch gespeichert sein sollten. Werte und Datensätze können verschieden sein, die eigentlich gleich seien sollten. Insgesamt gibt es drei Anomalien, die einfach durch die Normalisierungen verhindert werden können.

Die Einfüge-Anomalie ist eine Anomalie, die beim Einfügen von Daten in der Datenbank
auftritt. Es kann passieren, dass ein Primärschlüssel aus zwei Schlüssel besteht.
Da das Einfügen nun verlangt, dass beide Primärschlüssel vorhanden sind, führt dies zu Schwierigkeiten oder auch zu Fehlern, wenn nur ein Teil vom Primärschlüssel
besessen wird. 

Beispiel:
\begin{figure}[!htb]
	\centering
	\includegraphics[width=13cm,keepaspectratio]{bilder/KFZBeispiel}
	\caption{Einfüge-Anomalie Beispiel}
	\label{fig:kfzbeispiel}
\end{figure}


Es wird davon ausgegangen, dass die Schlüssel PersonId und Kennzeichen zusammen den Primärschlüssel ergeben, da eine Person mehrere Autos haben kann. Soll jetzt ein neues Auto eingegeben werden, führt das zu Problemen, denn ein Auto kann in diesem Beispiel nicht ohne eine Person existieren. In Abbildung \ref{fig:kfzbeispiel} ist der Ferrari ohne Person und hat damit keinen vollständigen Primärschlüssel. Dies würde auch anderes herum zu Problemen führen, da eine Person auch nicht ohne ein Auto existieren kann. Somit haben wir eine Einfüge-Anomalie.

Die Änderungs-Anomalie tritt beim Ändern von Daten in der Datenbank auf. Wenn 
viele redundante Daten in einer Tabelle sind und ein Name geändert werden soll, tritt diese Anomalie auf, wenn dieser Namen an vielen Stellen geändert werden müsste - Wobei in diesem Fall eine Änderung reichen sollte.

Beispiel:
\begin{figure}[!htb]
	\centering
	\includegraphics[width=13cm,keepaspectratio]{bilder/AlbumBeispiel}
	\caption{Lösch- und Änderungs-Anomalie Beispiel}
	\label{fig:albumbeispiel}
\end{figure}

Hier ist der Primärschlüssel die AlbumId. Die Probleme treten auf, wenn der Namen von Queen geändert werden würde. Normalerweise sollte eine Änderung reichen, um ein Attributwert zu ändern. In diesem Fall müsste jedes Album durchgegangen werden und da die Namen anpassen. Dieser extra Aufwand ist die Änderungs-Anomalie.

Die Lösch-Anomalie entsteht durch das Löschen von Daten aus der Datenbank.Wenn zwei unabhängige Daten gelöscht werden sollen und durch denn Aufbau beide Informationen gelöscht sind, entsteht eine Anomalie. Das Löschen sollte nur die Daten löschen, die auch beabsichtigt zu löschen sind. 

Beispiel aus Abbildung \ref{fig:albumbeispiel}:

Der Primärschlüssel ist immer noch AlbumId. Nun soll nichts mehr geändert, sondern gelöscht werden. Wenn jetzt das Album Arrival von ABBA gelöscht wird, würde damit auch direkt die Band ABBA gelöscht - obwohl nur beabsichtigt wurde, das Album zu löschen. 

Diese Anomalien werden alle durch die drei Normalformen beseitigt. 
Verschiedene Stufen der Normalformen sind:







Die erste Normalform besagt, dass jedes Attribut atomar sein soll. Damit ist gemeint, dass Daten zerkleinert werden sollen, die auch im einzelnen Aspekt eine Gewichtung haben, wie zum Beispiel Adressen. Adressen können als ein Ganzes zusammen geschrieben werden, aber Atomar wären sie erst, wenn sie in Straße, PLZ, Hausnummer und Stadt einteilt würden. Diese Einteilung verschafft nachher Vorteile, die für Datenbanken sinnvoll sind. Es kann einfacher nach Leuten gesucht werden, die alle in einer Stadt wohnen. Ohne diese atomaren Attribute müssten die Adressen für jede einzelne Suche zerlegen werden. Danach würde es erst möglich sein, die Städte zu vergleichen. Diese Regel wird leider in dieser Datenbank gebrochen, da die Namen von Autoren nicht in Vor- und Nachnamen geteilt sind.

In der DBLP sind die Autoren als ein ganzer Name gespeichert. Wären sie im Voraus getrennt gewesen, würden hier die Namen in Vor- und Nachname geteilt sein. Namen zu trennen ist eine Aufgabe für sich. Namen wie Jürgen von der Lippe oder Angela Dorothea Merkel haben schon verschiedene Trennungsmöglichkeiten. Hier ist zu sehen, dass es nicht einfach möglich ist, ab einer bestimmten Anzahl von schon eingelesenen Namen den Nachnamen zu bestimmen. Würde man den letzten Namen als Nachname festlegen, hat Jürgen von der Lippe nur einen Teil seines Nachname. Diese zu verallgemeinern, ist eine größere Aufgabe. Dazu kommen Namen aus Ostasien, die wiederum Vor-und Nachname andersherum aufschreiben, da der Familien Name zuerst genannt wird. Diese Aufteilung würde das Ausmaß dieser Arbeit überschreiten, sodass angenommen wird, dass Namen atomar sind. 

Für die zweite Normalform muss sie erst mal in der 1. Normalform sein und dazu darf kein Attribut, welches auch kein Primärschlüssel ist, eine Teilmenge in der Tabelle besitzen, die von dem Attribut abhängig ist. Demnach müssen Attribute vollständig von einem Primärschlüssel abhängig sein und nicht nur halb. Hierdurch wird auch die Einfüge-Anomalie gelöst, da es nun keine zwei einzelne Entitäten mehr gibt, die in eine Tabelle zusammen gefügt sind. Um dies zu verdeutlichen, wird erneut ein Beispiel angeführt:


\begin{small}
	Person(\uline{Id}, Name, Nachname, Kennzeichen, Automarke, Autotyp)
\end{small}


In dieser Tabelle wäre das Kennzeichen ein Primärschlüssel von einer Automarke und eines Autotyp und damit hat ein Nichtprimärattribut eine Teilmenge von Attributen, die von ihm abhängig ist. Zwar gehört das Auto dieser Person, aber es wäre nur vollständig abhängig von der Kombination der beiden Attribute Id und Kennzeichen. Damit ist diese Tabelle nicht in der zweiten Normalform, denn dafür müssen die Tabellen so aussehen:

\begin{small}
	Person(\uline{Id}, Name, Nachname, \dotuline{Kennzeichen})\newline
	Auto(\uline{Kennzeichen}, Automarke, Autotyp)
\end{small}


Für die dritte Normalform muss zunächst die Tabelle in der zweiten Normalform sein und dann darf kein Nichtprimärschlüssel transitiv von einem Schlüsselkandidaten abhängen. Schlüsselkandidaten sind Attribute, die als Primärschlüssel Infrage kommen. Durch diese Normalform werden direkt zwei Anomalien beseitigt, die Änderung-Anomalie und die Lösch-Anomalie. 

Um dies zu verdeutlichen, ein weiteres Beispiel:

\begin{small}
	Album(\uline{Id}, Albumtitel, Interpret, Gründungsjahr, Erscheinungsjahr)
\end{small}


Hier wäre das Gründungsjahr transitiv über Interpret abhängig, denn Gründungsjahr wäre erst über den Interpreten mit dem Album verlinkt und würde somit die Regel brechen. Demnach muss der Interpret eine eigene Entität werden. Damit sehen die Tabellen dann so aus:

\begin{small}
	Album(\uline{Id}, Albumtitel, Erscheinungsjahr, Interpret)\newline
	Künstler(\uline{Interpret}, Gründungsjahr)
\end{small}


Die Tabelle ist in der dritten Normalform, wenn man davon ausgeht, dass der Interpret eindeutig ist, ansonsten wird noch ein künstlicher Schlüssel eingefügt. 

In der dritten Normalform sind nun alle Anomalien beseitigt. Das sind die Grundeigenschaften, die eine Datenbank erfüllen muss. Diese Datenbank erfüllt alle Kriterien der dritten Normalform, wenn davon ausgegangen wird, dass Namen ungetrennt schon atomar sind, falls nicht wären sie in der nullten Normalform.


\newpage
\subsection{Datendefinition in der Datenbank}


Nun wurde der gesamte theoretische Teil dieser Datenbank beschrieben. Für die Praxis wird SQL verwendet, bei dem es sich um eine Datensprache handelt. Sie ist dafür da, Datenbanken zu erstellen und zu verwalten. Diese Sprache teilt sich in drei Hauptteile von Befehlsgruppen ein, DDL für die Struktur der Datenbank, DML für das Daten manipulieren und DQL für das auslesen der Daten. Diese drei Teile werden benötigt und noch im weiteren Verlauf erklärt.

Hierzu wird DDL - \textit{ausgeschrieben Data Definition Language} - verwendet. Diese Sprache wird benutzt, um die Datenstruktur, die theoretisch aufgebaut wurde, in der Datenbank anzulegen. Also alle Tabellen, Beziehungen, Datentypen u.Ä.


\begin{figure}[!htb]
	\oranget{CREATE TABLE} AUTORSCHREIBTBUCH( \newline
	KEY \purplet{VARCHAR(100)} \oranget{NOT NULL},\newline
	ID \purplet{INTEGER}        \oranget{NOT NULL},\newline
	\oranget{PRIMARY KEY}(KEY,ID),\newline
	\oranget{FOREIGN KEY}(KEY) \oranget{REFERENCES} AUTOR(KEY)\newline
	\oranget{ON DELETE} CASCADE \oranget{ON UPDATE} CASCADE,\newline
	\oranget{FOREIGN KEY}(ID) \oranget{REFERENCES} BUCH(ID)\newline
	\oranget{ON DELETE} CASCADE \oranget{ON UPDATE} CASCADE);\newline
	\caption{DDL Ausschnitt}
	\label{fig:ddlbeispiel}
\end{figure}


Abbildung \ref{fig:ddlbeispiel} zeigt einen typischen SQL Auschnitt, in der die Tabelle Autor-schreibt-Buch erstellt wird. Dabei ist zu beachten, dass SQL nicht case sensitive ist, also achtet die Sprache nicht auf Groß- und Kleinschreibung. Wie hier zu sehen ist, wird erst der Create-Befehl benutzt, sodass eine Tabelle namens AUTORSCHREIBTBUCH erstellt wird und in der dahinterliegenden Klammer die Tabellenstruktur kommt. Hier werden erst die beiden Attribute Key und ID erstellt. Hinter den jeweiligen Namen kommen die Datentypen in Varchar und Integer. Varchar steht für eine variable Zeichenkette mit einem Maximum an Zeichen. Variabel ist sie, da sie nicht immer die maximale Anzahl als Speicher einnimmt. Integer steht für eine Ganzzahl. Fasst alle Attribute sind in Varchar gespeichert, da die DBLP nicht genormt ist. Viele Werte, die eigentlich ein Integer wären, werden durch Ausnahmen wieder zu Varchar, bspw. gibt es bei Büchern das Attribut Auflage(Volume). Durchweg werden hier Ganzzahlen verwendet, die Auflagen beschreiben, aber manche Auflagen sind mit langen komplizierten Zeichenketten beschrieben, was Integer wieder hinfällig macht. Deshalb sind nur die künstlichen Schlüssel vom Typ Integer. Diese Tabelle ist die Relation zwischen Autor und Buch. Deshalb sind beide Attribute Fremdschlüssel(Foreign Key). Erst wird der Fremdschlüssel als ein solcher markiert und danach wird dieser auf die Tabelle mit dem Attribut, zu dem es gehört, referenziert. Dabei spielt es keine Rolle, ob der Fremdschlüssel denselben Namen hat wie der Primärschlüssel, den er repräsentiert. So könnte auch der Fremdschlüssel umbenannt werden und im Nachhinein trotzdem auf den richtigen Primärschlüssel verweisen. Es könnte zum Beispiel ID auf BuchId geändert werden, wozu nur die zwei ID’s umbenannt werden, aber dabei nicht die Referenz geändert muss.

Unter dem Foreign-Key-Befehl stehen noch optionale Eigenschaften zu Fremdschlüsseln. Hiermit wird beschrieben, was bei einer Änderung oder Löschung von dem Primärschlüssel in der referenzierten Tabelle passiert. Es gibt fünf Möglichkeiten, die eingestellt werden können:

NO ACTION - Hier wird nichts mit der Tabelle getan. Da dann aber ein Fremdschlüssel
auf einen Datensatz verweist, wird ein Fehler geworfen. Dies ist auch die Standardeinstellung, wenn keine Option gewählt wurde.

RESTRICT - Es wird verboten, den verwiesenen Primärschlüssel zu löschen.

CASSCADE - Der Tabelleneintrag wird gelöscht/geändert, falls der verwiesene Primärschlüssel gelöscht wird oder sich ändert.

SET NULL - Der Fremdschlüssel wird auf Null gesetzt, wenn der verwiesene Primärschlüssel gelöscht wird oder sich ändert.

SET DEFAULT - Wie bei SET NULL, nur hier wird der Fremdschlüssel auf den voreingestellten Standardwert gesetzt.

In diesem Fall werden beide Male CASSCADE benutzt, da dies eine Relationstabelle ist und diese die Änderungen und Löschungen mit übernehmen soll. Da diese Tabelle nur die Beziehung darstellt und keine wirklichen Daten besitzt, geschehen hiermit keine Löschungen oder Änderungen, die Fehler erzeugen könnten.


\subsection{Daten einfügen}

Nun wird die Datenbank, mit den Daten die schon mit dem DBParser angepasst wurden, befüllt. Hierfür wird Data Manipulation Language (DML) verwendet, also die Datenbearbeitungssprache, welche wieder einen Teil von SQL darstellt. Mit diesem Teil können Daten eingefügt, geändert oder auch gelöscht werden. In diesem Teil der Arbeit wird die Hauptaufgabe das Einfügen sein.

\begin{figure}[!htb]
	%\includegraphics[width=13cm,keepaspectratio]{bilder/DDLBeispiel}
	\oranget{INSERT INTO} AUTOR (KEY, NAME)\newline
	\oranget{VALUES} (???,???);\newline
	\caption{DML Ausschnitt}
	\label{fig:dmlbeispiel}
\end{figure}

In Abbildung \ref{fig:dmlbeispiel} wird ein Datensatz in die Autor-Tabelle eingetragen. Mit dem INSERT INTO Befehl wird bestimmt, dass Daten eingespeist werden sollen. Nach dem INTO kommt die Tabelle in welche gespeichert wird. In der Klammer sind dann die Attribute angegeben, die bei diesem Datensatz befüllt werden sollen. Hinter VALUES in der Klammer werden nun die Daten in der Reihenfolge eingefügt, wie sie in der ersten Klammer schon angegeben wurden, ein. Die Fragezeichen stellen in diesem Beispiel die Daten da. Da der Key ein Primärschlüssel ist, tritt ein Fehler auf, wenn zwei Mal der gleiche Wert dafür einfügt wird. Dadurch wird klar, dass der Datensatz schon vorhanden ist.












